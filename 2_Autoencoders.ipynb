{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAMicDsjNKBN"
   },
   "source": [
    "# Autoencoders\n",
    "\n",
    "In this Notebook we will be dealing (for most of the time) with the same Dataset as previously... but our task is different.\n",
    "\n",
    "Instead of classifying the images we will be reducing their dimensions into so called \"latent space\" and then, reconstruct them into images again. Our goal is to obtain the output which will be similar to the input as much as possible...\n",
    "\n",
    "Let's get familiar with a special group of Deep Neural Networks - Autoencoders!\n",
    "\n",
    "- Autoencoders are a specific type of feedforward Neural Networks where the input is the same as the output\n",
    "\n",
    "- They consist of 3 parts: **encoder**, **latent space** and **decoder**. \n",
    "\n",
    "- They compress the input into a lower-dimensional **latent space** (**encoder**) and then reconstruct the output from this representation (**decoder**).\n",
    "\n",
    "- The role of basic Autoencoder is mostly dimensionality reduction. \n",
    "\n",
    "**TASK:** Train the simple Autoencoder model for 2 different types of data. \n",
    "\n",
    "1. Linear Autoencoder for MNIST dataset\n",
    "2. Convolutional Autoencoder for MNIST dataset\n",
    "3. **Exercise:** Build Autoencoder for real physics data!\n",
    "\n",
    "**WHAT YOU WILL LEARN:**\n",
    "\n",
    "- what are main components of Autoencoder\n",
    "- how to implement Linear Autoencoder\n",
    "- how to implement Convolutional Autoencoder\n",
    "- how to add 1d and 2d Normalization layers to the model architecture\n",
    "- how to build Dataset class for custom dataset\n",
    "- how to implement Autoencoder for real data\n",
    "\n",
    "\n",
    "**TO DO:** Read and understand the following code. Run the cells, analyse the results and if everything is clear, follow the instructions concerning exercises parts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aTPv26RqAXyc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid \n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7YrgLXrTUguV"
   },
   "source": [
    "## 1. Autoencoder Linear\n",
    "\n",
    "We will build the simple Linear Autoencoder to decode and encode popular dataset of images of handwritted digits. \n",
    "\n",
    "In order to test the efficiency of our algorithm we will compare orginal data with decoded data (visually)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLdXeh3WdyXs"
   },
   "source": [
    "### Data preprocessing \n",
    "\n",
    "Let's download our MNIST dataset and create DataLoader. In this case, we don't need two separated datasets (train and test), since we are dealing with unsupervised learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lerr1BMqdHsI"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),])\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "mnist_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_data,\n",
    "                                          batch_size=64,\n",
    "                                          shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DWVKnLNlfvlc"
   },
   "source": [
    "### Build the Autoencoder\n",
    "\n",
    "In order to build an autoencoder we need 3 things: an encoding method, a decoding method, and a loss function to compare the output with the target. \n",
    "\n",
    "Both the encoder and decoder are fully-connected feedforward neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pcURaVrVIXZS"
   },
   "outputs": [],
   "source": [
    "# Defining Model\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Role of encoder: repeatedly reduce the size\n",
    "        # N - batch size\n",
    "        # input dimensions: 28x28 = 784\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 128), # (N, 784) -> (N, 128)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 12),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(12, 3) # -> N, 3 - our Latent Space dimension is 3\n",
    "        )\n",
    "        # Role of decoder: repeatedly increase the size\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3, 12),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(12, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 28 * 28),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "model = Autoencoder()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOZRz7EjNNQe"
   },
   "source": [
    "### Define Loss and Optimizer\n",
    "\n",
    "Loss function: we use mean squared error (MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y7M83DlnIckZ"
   },
   "outputs": [],
   "source": [
    "model = Autoencoder()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=1e-3, \n",
    "                             weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ze0ZusdgCbo"
   },
   "source": [
    "### Main training loop\n",
    "\n",
    "We will train our Autoencoder with only 1 epoch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gbf2a8JUSOVb"
   },
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "outputs = []\n",
    "for epoch in range(num_epochs):\n",
    "    for (img, _) in data_loader:\n",
    "        img = img.reshape(-1, 28*28) # we need to reshape 28x28 image into vector\n",
    "        recon = model(img)\n",
    "        loss = criterion(recon, img)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch:{epoch+1}, Loss:{loss.item():.4f}')\n",
    "    outputs.append((epoch, img, recon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNSZH-migLL1"
   },
   "source": [
    "### Compare original images with reconstructed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99YSPlwOSiPT"
   },
   "outputs": [],
   "source": [
    "for k in range(0, num_epochs):\n",
    "    plt.figure(figsize=(9, 2))\n",
    "    plt.gray()\n",
    "    imgs = outputs[k][1].detach().numpy()\n",
    "    recon = outputs[k][2].detach().numpy()\n",
    "    for i, item in enumerate(imgs):\n",
    "        if i >= 9: break\n",
    "        plt.subplot(2, 9, i+1)\n",
    "        item = item.reshape(-1, 28,28) # -> use for Autoencoder_Linear\n",
    "        # item: 1, 28, 28\n",
    "        plt.imshow(item[0])\n",
    "\n",
    "            \n",
    "    for i, item in enumerate(recon):\n",
    "        if i >= 9: break\n",
    "        plt.subplot(2, 9, 9+i+1) # row_length + i + 1\n",
    "        item = item.reshape(-1, 28,28) # -> use for Autoencoder_Linear\n",
    "        # item: 1, 28, 28\n",
    "        plt.imshow(item[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CANNjnml17WI"
   },
   "source": [
    "### **Exercise**\n",
    "\n",
    "**Normalization layers**\n",
    "\n",
    "Normalizing a set of data transforms the set of data to be on a similar scale. It may help to stabilize the process of training.\n",
    "It is more useful for complex architectures with many layers, however let's learn how to add them in this simple example.\n",
    "**Batch normalization** standardizes the input of a layer across a single batch.\n",
    "\n",
    "Modify the architecture of the Autoencoder by adding `nn.BatchNorm1d(out)` layer after each Linear layer, where the `out` parameter should be equal to the size of the output of the previous Linear layer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RpWBI1Gv2UER"
   },
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Role of encoder: repeatedly reduce the size\n",
    "        # N - batch size\n",
    "        # input dimensions: 28x28 = 784\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 128), # (N, 784) -> (N, 128)\n",
    "            ## add BatchNorm1d layer here\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            ## add BatchNorm1d layer here\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 12),\n",
    "            ## add BatchNorm1d layer here\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(12, 3) # -> N, 3\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3, 12),\n",
    "            ## add BatchNorm1d layer here\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(12, 64),\n",
    "            ## add BatchNorm1d layer here\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            ## add BatchNorm1d layer here\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 28 * 28),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "model = Autoencoder()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hARZcdGeie9M"
   },
   "source": [
    "####  Train your new architecture..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UVCM1xGL2jAu"
   },
   "outputs": [],
   "source": [
    "model = Autoencoder()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=1e-3, \n",
    "                             weight_decay=1e-5)\n",
    "\n",
    "\n",
    "num_epochs = 1\n",
    "outputs = []\n",
    "for epoch in range(num_epochs):\n",
    "    for (img, _) in data_loader:\n",
    "        img = img.reshape(-1, 28*28) # for Autoencoder_Linear we need to reshape 28x28 image into vector\n",
    "        recon = model(img)\n",
    "        loss = criterion(recon, img)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch:{epoch+1}, Loss:{loss.item():.4f}')\n",
    "    outputs.append((epoch, img, recon))\n",
    "\n",
    "\n",
    "\n",
    "for k in range(0, num_epochs):\n",
    "    print(f'Epoch {k}:')\n",
    "    plt.figure(figsize=(9, 2))\n",
    "    plt.gray()\n",
    "    imgs = outputs[k][1].detach().numpy()\n",
    "    recon = outputs[k][2].detach().numpy()\n",
    "    for i, item in enumerate(imgs):\n",
    "        if i >= 9: break\n",
    "        plt.subplot(2, 9, i+1)\n",
    "        item = item.reshape(-1, 28,28) # -> use for Autoencoder_Linear\n",
    "        # item: 1, 28, 28\n",
    "        plt.imshow(item[0])\n",
    "\n",
    "            \n",
    "    for i, item in enumerate(recon):\n",
    "        if i >= 9: break\n",
    "        plt.subplot(2, 9, 9+i+1) # row_length + i + 1\n",
    "        item = item.reshape(-1, 28,28) # -> use for Autoencoder_Linear\n",
    "        # item: 1, 28, 28\n",
    "        plt.imshow(item[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Df9RnvQiU8Rh"
   },
   "source": [
    "## 2. Autoencoder Conv\n",
    "\n",
    "Since we are working with image data - let's use convolutions!\n",
    "\n",
    "**Why convolutions?**\n",
    "\n",
    "A convolutional layer is much more specialized and efficient, than a fully connected layer.\n",
    "\n",
    "In a fully connected layer each neuron is connected to every neuron in the previous layer, and each connection has its own weight. A neuron in a convolutional layer, however, is only connected to \"nearby\" neurons from the preceding layer within the width of the convolutional kernel.\n",
    "\n",
    "\n",
    "Typical use case for convolutional layers is for image data where, as required, the features are local (e.g. a \"nose\" consists of a set of nearby pixels, not spread all across the image).\n",
    "\n",
    "The fewer number of connections and weights make convolutional layers relatively cheap (vs full connect) in terms of memory and compute power needed.\n",
    "\n",
    "Read more about [components of convolution layers (kernel size, padding, stride)](https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148)\n",
    "\n",
    "**TO DO:**\n",
    "Run and analyse the following code with Autoencoder implemented with convolutional layers. Pay special attention to the dimensions of input and output of each layer (as always!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQZwFUyihKvD"
   },
   "source": [
    "**Conv vs Transposed Conv**\n",
    "\n",
    "We will use 2 types of convolutional layers. \n",
    "\n",
    "1) `Conv2d` - A convolutional layer extracts features from the layer, and downsamples the input.\n",
    "\n",
    "\n",
    "2) `ConvTranspose2d` - In a tranposed convolution, instead of the input being larger than the output, the output is larger. \n",
    "A transposed convolutional layer attempts to reconstruct the spatial dimensions of the convolutional layer and reverses the downsampling and upsampling techniques applied to it.\n",
    "\n",
    "Read more about [transposed convolutions](https://medium.com/@marsxiang/convolutions-transposed-and-deconvolution-6430c358a5b6).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SqUJZYPwVBWb"
   },
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()        \n",
    "        # input: N, 1, 28, 28\n",
    "        # N - batch size\n",
    "        # 1 - number of channels (RGB images would have 3 channels)\n",
    "        # 28 x 28 - dimesions of input images\n",
    "        # role of encoder: from N,1,28,28 -> N,64,1,1\n",
    "        # Conv2d: in_channels, out_channels, ks, stride, padding\n",
    "        # New dimensions (n x n) of each layer can be calculated from the formula: (n+2*pad - ks)//stride + 1\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=2, padding=1), # -> (28 + 2*1 - 3)//2 + 1 = 14x14 (ch: 1 -> 16) -> N, 16, 14, 14\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1), # -> (14+2*1 - 3)//2 + 1 = 7x7 (ch: 16 -> 32) -> N, 32, 7, 7\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 7, stride=1, padding=0) # -> (7+2*0 - 7)//1 + 1 = 1x1 (ch: 32 -> 64) -> N, 64, 1, 1\n",
    "        )\n",
    "        \n",
    "        # input: N, 64, 1, 1\n",
    "        # N - batch size\n",
    "        # 64 - number of channels\n",
    "        # 1 x 1 - dimesions\n",
    "        # role of encoder: from N,64,1,1 -> N,1,28,28 (back to original!)\n",
    "        # ConvTranspose2d: in_channels, out_channels, ks, stride, padding, output_padding\n",
    "        # New dimensions (n) of each layer can be calculated from the formula: (n-1)*stride - 2*padding + ks + out_padding\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, 7, stride=1, padding=0), # -> (1-1)*1 - 2*0 + 7 = 7x7 (ch: 32 -> 64) -> N, 32, 7, 7\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1), # -> (7-1)*2 - 2*1 + 3 + 1 = 14x14 (ch: 32 -> 16) -> N, 16, 14, 14 \n",
    "            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1), # -> (14-1)*2 - 2*1 + 3 + 1 = 28x28 (ch: 16 -> 1) -> N, 1, 28, 28 \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tLicsRr1VQ2s"
   },
   "outputs": [],
   "source": [
    "model = Autoencoder()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=1e-3, \n",
    "                             weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GS6N7rNPVS2c",
    "outputId": "91e070f0-3a6d-48e2-d480-10ac5c4be2a8"
   },
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "outputs = []\n",
    "for epoch in range(num_epochs):\n",
    "    for (img, _) in data_loader:\n",
    "        recon = model(img)\n",
    "        loss = criterion(recon, img)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch:{epoch+1}, Loss:{loss.item():.4f}')\n",
    "    outputs.append((epoch, img, recon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZXSAAkvhVUze"
   },
   "outputs": [],
   "source": [
    "for k in range(0, num_epochs):\n",
    "    plt.figure(figsize=(9, 2))\n",
    "    plt.gray()\n",
    "    imgs = outputs[k][1].detach().numpy()\n",
    "    recon = outputs[k][2].detach().numpy()\n",
    "    for i, item in enumerate(imgs):\n",
    "        if i >= 9: break\n",
    "        plt.subplot(2, 9, i+1)\n",
    "        # item: 1, 28, 28\n",
    "        plt.imshow(item[0])\n",
    "            \n",
    "    for i, item in enumerate(recon):\n",
    "        if i >= 9: break\n",
    "        plt.subplot(2, 9, 9+i+1) # row_length + i + 1\n",
    "        # item: 1, 28, 28\n",
    "        plt.imshow(item[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYywlUhBtLrV"
   },
   "source": [
    "### **Exercise**\n",
    "\n",
    "1. Compare the results of Linear and Convolutional Autoencoders (visible results, loss value, time of training). You may experiment with a larger numbers of epoch for both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldcPvtU19NN7"
   },
   "source": [
    "2. Similarly to the example with Linear Autoencoder - add some Normalization layers. REMARK - use `nn.BatchNorm2d(out)` after each Conv2d or Conv2dTranspose layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KzJVCN5f9x2P"
   },
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=2, padding=1), # -> N, 16, 14, 14\n",
    "            ## add BatchNorm2d layer here\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1), # -> N, 32, 7, 7\n",
    "            ## add BatchNorm2d layer here\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 7, stride=1, padding=0), # -> N, 64, 1, 1\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, 7, stride=1, padding=0), #  -> N, 32, 7, 7\n",
    "            ## add BatchNorm2d layer here\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1), # -> N, 16, 14, 14\n",
    "            ## add BatchNorm2d layer here\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1), # -> N, 1, 28, 28 \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "model = Autoencoder()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=1e-3, \n",
    "                             weight_decay=1e-5)\n",
    "\n",
    "\n",
    "num_epochs = 1\n",
    "outputs = []\n",
    "for epoch in range(num_epochs):\n",
    "    for (img, _) in data_loader:\n",
    "        recon = model(img)\n",
    "        loss = criterion(recon, img)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch:{epoch+1}, Loss:{loss.item():.4f}')\n",
    "    outputs.append((epoch, img, recon))\n",
    "\n",
    "\n",
    "for k in range(0, num_epochs):\n",
    "    plt.figure(figsize=(9, 2))\n",
    "    plt.gray()\n",
    "    imgs = outputs[k][1].detach().numpy()\n",
    "    recon = outputs[k][2].detach().numpy()\n",
    "    print(\"Original data\")\n",
    "    for i, item in enumerate(imgs):\n",
    "        if i >= 9: break\n",
    "        plt.subplot(2, 9, i+1)\n",
    "        # item: 1, 28, 28\n",
    "        plt.imshow(item[0])\n",
    "    print(\"Reconstructed data\")\n",
    "    for i, item in enumerate(recon):\n",
    "        if i >= 9: break\n",
    "        plt.subplot(2, 9, 9+i+1) # row_length + i + 1\n",
    "        # item: 1, 28, 28\n",
    "        plt.imshow(item[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XhAzW4ZxtiNT"
   },
   "source": [
    "## 3. Build Autoencoder for real physics data!\n",
    "\n",
    "Time for the main part of our Autoencoder Exercise! Let's try to use Autoencoder for reconstruction the data from Monte Carlo simulation of photon beam therapy (PBT). \n",
    "\n",
    "**About the data**\n",
    "\n",
    "The data are so called phase spaces from Monte Carlo simulation of photon beam therapy (PBT). \n",
    "They contain information about dose delivery in a plane perpendicular to the axis of a beam of ionizing radiation. \n",
    "\n",
    "The phase space is a collection of particles (photons, electrons, and positrons) — for each particle its energy, momentum, and coordinates of the point of crossing the phase space plane are stored. \n",
    "\n",
    "\n",
    "Out data is collection of 995401 Photons. \n",
    "Photons in a phase space are described by six parameters: \n",
    "- position coordinates X, Y in the plane perpendicular to the beam axis\n",
    "- direction of momentum (dX, dY, dZ) which is a unit vector\n",
    "- energy (E) of the photon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vNKqjO_pJ-9F"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler\n",
    "from typing import List, Tuple, Type\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import random\n",
    "\n",
    "CUDA_DEVICE_NUM=0\n",
    "DEVICE = torch.device(f'cuda:{CUDA_DEVICE_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0X0hK-FVLE1X"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "RANDOM_SEED = 123\n",
    "LEARNING_RATE = 0.0005\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tscEzR04t6Ui"
   },
   "source": [
    "### Prepare and show the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rAyNM4t4GRTn"
   },
   "outputs": [],
   "source": [
    "!wget https://csc-files.web.cern.ch/CSC2022/photons.npy\n",
    "\n",
    "path = 'photons.npy'\n",
    "\n",
    "photons = np.load(path)\n",
    "X = np.zeros(photons.shape,dtype=np.float32)\n",
    "np.copyto(X,photons)\n",
    "\n",
    "df_data = pd.DataFrame(X, columns = ['X', 'Y', 'dX', 'dY', 'dZ', 'E'])\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cm2jEaDYGn5d"
   },
   "outputs": [],
   "source": [
    "print(df_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4iiu9Tu7W68"
   },
   "source": [
    "We will plot histograms of all parameters of particles: energy, momentum, and coordinates of the point of crossing the phase space plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6lNBvy2wmD_t"
   },
   "outputs": [],
   "source": [
    "tmp=df_data.to_numpy(dtype=np.float32)\n",
    "#print(tmp)\n",
    "tmp=tmp[:,:]\n",
    "orginal=copy.deepcopy(tmp)\n",
    "\n",
    "fig, axs = plt.subplots(2, 3)\n",
    "fig.set_size_inches(16,8)\n",
    "bins = 100\n",
    "axs[0, 0].hist(orginal[:,0],bins=bins, label ='orginal',alpha=0.5)\n",
    "axs[0, 0].set_title('E')\n",
    "axs[0, 1].hist(orginal[:,1],bins=bins, label ='orginal',alpha=0.5)\n",
    "axs[0, 1].set_title('X')\n",
    "axs[0, 2].hist(orginal[:,2],bins=bins, label ='orginal',alpha=0.5)\n",
    "axs[0, 2].set_title('Y')\n",
    "axs[1, 0].hist(orginal[:,3],bins=bins, label ='orginal',alpha=0.5)\n",
    "axs[1, 0].set_title('dX')\n",
    "axs[1, 1].hist(orginal[:,4],bins=bins, label ='orginal',alpha=0.5)\n",
    "axs[1, 1].set_title('dY')\n",
    "axs[1, 2].hist(orginal[:,5],bins=bins, label ='orginal',alpha=0.5)\n",
    "axs[1, 2].set_title('dZ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-Paak2X9PYh"
   },
   "source": [
    "Let's build dedicated PhotonsDataset class that inherits from Dataset. From this point we will be able to create DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35UTTQMuJV2L"
   },
   "outputs": [],
   "source": [
    "class PhotonsDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.x = data\n",
    "        self.n_samples = len(data)\n",
    "        self.transform = transform\n",
    "        self.columns = ['X', 'Y', 'dX', 'dY', 'dZ', 'E']\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.x\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "\n",
    "class ToTensorFromNdarray:\n",
    "    def __call__(self, sample):\n",
    "        # Returns a tensor made from ndarray\n",
    "        return torch.from_numpy(sample)\n",
    "\n",
    "photons = np.load(path)\n",
    "train_transforms = ToTensorFromNdarray()\n",
    "train_dataset = PhotonsDataset(\n",
    "        data=photons, transform=train_transforms)\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                                  batch_size=BATCH_SIZE,\n",
    "                                  num_workers=2,\n",
    "                                  shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dsYuwZkgLsi2",
    "outputId": "1646995a-d470-42b3-f2ce-25d5638dbe61"
   },
   "outputs": [],
   "source": [
    "print('Training Set:\\n')\n",
    "for features in train_loader:  \n",
    "    print('Batch dimensions:', features.size())\n",
    "    print(features[:4,:6])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yEEAbeKouEiH"
   },
   "source": [
    "### **Exercise**: Build the Autoencoder\n",
    "\n",
    "Try to design the Autoencoder that will be able to process our dataset with Photons. The rest (training loop, loss, optimizers) are already coded in next cells. Your role is to put layers into the encoder and decoder!\n",
    "\n",
    "Hints:\n",
    "1. Will you use Linear or Conv version of Autoencoder? \n",
    "2. What are input dimensions of our data?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8U6Thxmjt_Mp"
   },
   "outputs": [],
   "source": [
    "class Autoencoder_Linear(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()        \n",
    "        self.encoder = nn.Sequential(\n",
    "            ### YOUR CODE HERE\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            ### YOUR CODE HERE\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded, encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fn-QrYKVNIXC"
   },
   "outputs": [],
   "source": [
    "model = Autoencoder_Linear()\n",
    "model.to(DEVICE)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=1e-3, \n",
    "                             weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_c8lxKC6PehX"
   },
   "outputs": [],
   "source": [
    "outputs = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for features in train_loader:\n",
    "        features = features.to(DEVICE)\n",
    "        recon, _ = model(features)\n",
    "        loss = criterion(recon, features)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch:{epoch+1}, Loss:{loss.item():.4f}')\n",
    "    outputs.append((epoch, img, recon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tr_TwjRWWOs0"
   },
   "outputs": [],
   "source": [
    "tmp=df_data.to_numpy(dtype=np.float32)\n",
    "tmp=tmp[:,:]\n",
    "orginal=copy.deepcopy(tmp)\n",
    "tmp=torch.from_numpy(tmp)\n",
    "with torch.no_grad():\n",
    "    result, result_encoded_features=model(tmp.to(device=DEVICE))\n",
    "result=result.cpu().detach().numpy() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NOCqKyJAAk_i"
   },
   "source": [
    "Let's see how our model works by comparing histograms with reconstructed data!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NEJpzSdDWPx3"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 3)\n",
    "fig.set_size_inches(16,8)\n",
    "bins = 100\n",
    "axs[0, 0].hist(orginal[:,0],bins=bins, label ='orginal',alpha=0.5)\n",
    "axs[0, 0].hist(result[:,0],bins=bins, label ='encoded', alpha=0.5)\n",
    "axs[0, 0].set_title('E')\n",
    "axs[0, 1].hist(orginal[:,1],bins=bins, label ='orginal',alpha=0.5)\n",
    "axs[0, 1].hist(result[:,1],bins=bins, label ='encoded', alpha=0.5)\n",
    "axs[0, 1].set_title('X')\n",
    "axs[0, 2].hist(orginal[:,2],bins=bins, label ='orginal',alpha=0.5)\n",
    "axs[0, 2].hist(result[:,2],bins=bins, label ='encoded', alpha=0.5)\n",
    "axs[0, 2].set_title('Y')\n",
    "axs[1, 0].hist(orginal[:,3],bins=bins, label ='orginal',alpha=0.5)\n",
    "axs[1, 0].hist(result[:,3],bins=bins, label ='encoded', alpha=0.5)\n",
    "axs[1, 0].set_title('dX')\n",
    "axs[1, 1].hist(orginal[:,4],bins=bins, label ='orginal',alpha=0.5)\n",
    "axs[1, 1].hist(result[:,4],bins=bins, label ='encoded', alpha=0.5)\n",
    "axs[1, 1].set_title('dY')\n",
    "axs[1, 2].hist(orginal[:,5],bins=bins, label ='orginal',alpha=0.5)\n",
    "axs[1, 2].hist(result[:,5],bins=bins, label ='encoded', alpha=0.5)\n",
    "axs[1, 2].set_title('dZ')\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D_A4JwxUBHee"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HP9QoGeNXdYO"
   },
   "source": [
    "### **Exercise**\n",
    "\n",
    "(Optional) Try to make your results better (experiment with deeper architecture, different dimension of latent space, hyperparameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1b5dpSB-Xw08"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
