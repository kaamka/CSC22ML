{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# GANs (Generative Adversarial Networks)\n",
        "\n",
        "Now its time for more advanced types of neural networks: GANs!\n",
        "Generative adversarial networks (GANs) are deep learning architectures that use two neural networks, pitting one against the other in order to generate new instances of data. They are used widely in new images generation. \n",
        "\n",
        "Very famous application of GANs was [Face Generator (This Person Does Not Exist)](https://this-person-does-not-exist.com/en).\n",
        "\n",
        "**TASK:** Generating totally new images with handwritten images with GANs\n",
        "\n",
        "\n",
        "**WHAT YOU WILL LEARN:**\n",
        "\n",
        "- how to implement basic GAN\n",
        "- how to implement loss function for GANs (min max game) using built-in cross entropy pytorch implementation\n",
        "- how to plot loss for Generator and Discriminator\n",
        "- what is the importance of Normalization Layers in more complex architectures like GAN and how it affects the training stability\n",
        "- how make your code more modular (especially useful for very big and complex architectures)\n",
        "\n",
        "\n",
        "**TO DO:** Read and understand following code. Run the cells, analyse the results and if everything is clear, follow the instructions concerning exercises part."
      ],
      "metadata": {
        "id": "nuwBA9-hqNCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.utils import make_grid\n",
        "from tqdm.auto import tqdm\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "WmdEzExCLrL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The simplest GANs architecture\n",
        "\n"
      ],
      "metadata": {
        "id": "gcL1Vml1zUoL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data \n",
        "\n",
        "In this part, we will build the function for showing the image from a tensor, that represents image (digit).\n",
        "\n",
        "Then we will create DataLoader from MNIST Dataset. Similarly to Autoencoders we don't need two separated datasets (train and test), since we are dealing with unsupervised learning!"
      ],
      "metadata": {
        "id": "WpdLcpNmziM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show(tensor, ch=1, size=(28,28), num=16):\n",
        "  # tensor: 128 x 784\n",
        "  data=tensor.detach().cpu().view(-1,ch,*size) # 128 x 1 x 28 x 28\n",
        "  grid = make_grid(data[:num], nrow=4).permute(1,2,0)   # 1 x 28 x 28  = 28 x 28 x 1\n",
        "  plt.imshow(grid)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "55O8Pzz3RInj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transforms = transforms.Compose(\n",
        "    [transforms.ToTensor(), \n",
        "     ]\n",
        ")\n",
        "\n",
        "dataset = datasets.MNIST(root=\"dataset/\", transform=transforms, download=True)\n"
      ],
      "metadata": {
        "id": "sJDCoho2sYM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameters and other settings\n"
      ],
      "metadata": {
        "id": "dYMqSWJcsYfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20\n",
        "LEARNING_RATE = 0.00001 # learning rate - speed of tweak the params of neural network\n",
        "# LEARNING_RATE = 0.1\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "\n",
        "cur_step = 0\n",
        "info_step = 300 # show information about the current loss values and visualisation\n",
        "mean_gen_loss = 0\n",
        "mean_disc_loss = 0\n",
        "z_dim = 64 #latent space dimension\n",
        "\n",
        "#device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device = \"cpu\"\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# number of steps: 60000 / 128  = 468.75\n"
      ],
      "metadata": {
        "id": "1yCgqTZVsXmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generator Model\n",
        "\n",
        "The key part - implementing the Generator Network.\n",
        "\n",
        "This time, in order to make the code more modular, we create `genBlock()`, that consists of sequence of layers: (`Linear`, `ReLU`) instead of 'rewriting' it few times. It is useful especially for more complex architectures.\n",
        "\n",
        "\n",
        "Pay special attention to input and output dimensions of each layer!\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "P9lnEdLv7OW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, z_dim=64, i_dim=784, h_dim=128):\n",
        "        super().__init__()\n",
        "        self.gen = nn.Sequential(\n",
        "        self.genBlock(z_dim, h_dim), # 64, 128\n",
        "        self.genBlock(h_dim, h_dim*2), # 128, 256\n",
        "        self.genBlock(h_dim*2, h_dim*4), # 256 x 512\n",
        "        self.genBlock(h_dim*4, h_dim*8), # 512, 1024\n",
        "        nn.Linear(h_dim*8, i_dim), # 1024, 784 (28x28)\n",
        "        nn.Sigmoid(),\n",
        "    )\n",
        "\n",
        "    def genBlock(self, inp, out):\n",
        "        return nn.Sequential(\n",
        "        nn.Linear(inp, out),\n",
        "        #nn.BatchNorm1d(out),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "\n",
        "    def forward(self, noise):\n",
        "       return self.gen(noise)\n",
        "\n",
        "def gen_noise(number, z_dim):\n",
        "  return torch.randn(number, z_dim).to(device)\n",
        "\n",
        "gen = Generator(z_dim).to(device)\n",
        "gen"
      ],
      "metadata": {
        "id": "wR0B8dwjMBXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GAN takes random noise as its input. The generator then transforms this noise into a meaningful output.\n",
        "\n",
        "\n",
        "Let's generate some noise and pass it to the Generator. Since, the Generator is not trained yet, the output should stil look like a noise."
      ],
      "metadata": {
        "id": "txmosKbcgl2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x,y = next(iter(dataloader))\n",
        "print(x.shape, y.shape)\n",
        "print(y[:10])\n",
        "show(x)\n",
        "\n",
        "noise = gen_noise(BATCH_SIZE, z_dim)\n",
        "fake = gen(noise)\n",
        "show(fake)"
      ],
      "metadata": {
        "id": "EDJj7SpWiMdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Discriminator Model\n",
        "\n",
        "The key part - implementing the Discriminator Network.\n",
        "\n",
        "\n",
        "This time, in order to make the code more modular, we create `discBlock()`, that consists of sequence of layers: (`Linear`, `LeakyReLU`) instead of 'rewriting' it few times. Useful especially for more complex architectures!\n",
        "\n",
        "\n",
        "Pay special attention to input and output dimensions of each layer!"
      ],
      "metadata": {
        "id": "QsG_sFPd7QS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, i_dim=784, h_dim=256):\n",
        "        super().__init__()\n",
        "        self.disc = nn.Sequential(\n",
        "            self.discBlock(i_dim, h_dim*4), # 784 (28x28) -> 1024\n",
        "            self.discBlock(h_dim*4, h_dim*2), # 1024 -> 512\n",
        "            self.discBlock(h_dim*2, h_dim), # 512 -> 256\n",
        "            nn.Linear(h_dim, 1), # 256 -> 1\n",
        "            nn.Sigmoid() # get values between 0 and 1\n",
        "        )\n",
        "\n",
        "    def discBlock(self, inp, out):\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(inp, out),\n",
        "            nn.LeakyReLU(0.2)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, image):\n",
        "        return self.disc(image)\n",
        "\n",
        "\n",
        "disc = Discriminator().to(device)\n",
        "disc"
      ],
      "metadata": {
        "id": "oYzA-W8JMdKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimizer \n",
        "\n",
        "Initialize optimizers for models with previously selected value of learning rate."
      ],
      "metadata": {
        "id": "tOJvbHikfRDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gen_opt = torch.optim.Adam(gen.parameters(), lr=LEARNING_RATE)\n",
        "disc_opt = torch.optim.Adam(disc.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "AlLJj4_VhiiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generator Loss and Discriminator Loss\n",
        "\n",
        "The most challenging part of designing GANs is to implement a Loss function.\n",
        "We will use Cross Entropy here. \n",
        "\n",
        "**CROSS ENTROPY** - a measure of the difference between two probability distributions.\n",
        "\n",
        "Binary cross entropy is implemented in Pytorch as `nn.BCELoss().`\n",
        "\n"
      ],
      "metadata": {
        "id": "7RXcIys57Wq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_func = nn.BCELoss()\n",
        "#loss_func = nn.BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "Bqv91gBQPf95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#generator loss: log(1 - D(G(z))) <-> max log(D(G(z))\n",
        "def calc_gen_loss(loss_func, gen, disc, number, z_dim):\n",
        "    noise = gen_noise(number, z_dim) #number = batch size\n",
        "    fake = gen(noise)\n",
        "    pred = disc(fake) #predictions = output from discriminator 0-fake, 1-real\n",
        "    target = torch.ones_like(pred) #vector with ones with dimensionality like pred\n",
        "    gen_loss = loss_func(pred, target)\n",
        "    return gen_loss \n",
        "\n",
        "#discriminator loss: log(D(x)) + log(1 - D(G(z)))\n",
        "def calc_disc_loss(loss_func, gen, disc, number, real, z_dim):\n",
        "    noise = gen_noise(number, z_dim)\n",
        "    fake = gen(noise)\n",
        "    #detach fake images from the calculations when the discriminator is trained\n",
        "    disc_fake = disc(fake.detach()) \n",
        "    disc_fake_targets = torch.zeros_like(disc_fake)\n",
        "    disc_fake_loss = loss_func(disc_fake, disc_fake_targets)\n",
        "\n",
        "    disc_real = disc(real)\n",
        "    disc_real_targets = torch.ones_like(disc_real)\n",
        "    disc_real_loss = loss_func(disc_real, disc_real_targets)\n",
        "    \n",
        "    disc_loss = (disc_fake_loss + disc_real_loss)/2     \n",
        "    return disc_loss"
      ],
      "metadata": {
        "id": "RhtqVcv6v24P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Loop\n"
      ],
      "metadata": {
        "id": "4KJ8MJxK7ZOH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time for main training loop. In case of GANs you will need to wait for at least 10 epochs  in order to see reasonable output. \n",
        "After each epoch we will plot samples generated by our Generator. You can observe the process, where Model's output becomes better and better.\n",
        "You can start to think about exercises in the meantime."
      ],
      "metadata": {
        "id": "7CsQhulfQPFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gen_losses = []\n",
        "disc_losses = []\n",
        "epochs = 20\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for real,_ in dataloader:\n",
        "\n",
        "        #discriminator\n",
        "        disc_opt.zero_grad()\n",
        "\n",
        "        cur_bs = len(real) # real 128 x 1 x 28 x 28\n",
        "        real = real.view(cur_bs, -1) # 128 x 784\n",
        "        real = real.to(device)\n",
        "\n",
        "        disc_loss = calc_disc_loss(loss_func,gen,disc,cur_bs,real,z_dim)\n",
        "\n",
        "        disc_loss.backward(retain_graph=True)\n",
        "        disc_opt.step()\n",
        "        disc_losses.append(disc_loss.item())\n",
        "\n",
        "        #generator\n",
        "        gen_opt.zero_grad()\n",
        "        gen_loss = calc_gen_loss(loss_func, gen, disc, cur_bs, z_dim)\n",
        "        gen_loss.backward(retain_graph=True)\n",
        "        gen_opt.step()\n",
        "        gen_losses.append(gen_loss.item())\n",
        "\n",
        "        #visualisationa & stats\n",
        "        mean_disc_loss+=disc_loss.item()/info_step\n",
        "        mean_gen_loss+=gen_loss.item()/info_step\n",
        "\n",
        "        if cur_step % info_step == 0  and cur_step > 0:\n",
        "            fake_noise = gen_noise(cur_bs, z_dim)\n",
        "            fake = gen(fake_noise)\n",
        "            show(fake)\n",
        "            #show(real)\n",
        "            print(f\"Epoch: {epoch}: step {cur_step} / Gen loss: {mean_gen_loss} / Disc loss: {mean_disc_loss}\")\n",
        "\n",
        "\n",
        "            mean_gen_loss, mean_disc_loss = 0,0\n",
        "        cur_step+=1\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.semilogy(gen_losses, label=\"Generator Loss\")\n",
        "plt.semilogy(disc_losses, label=\"Discriminator Loss\")\n",
        "plt.grid(True, \"both\", \"both\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "YWdj6VQ4QFlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercises\n",
        "1. Look at your results. If you can't see any digits yet - let's try to fix our Model. Add `BatchNorm1d` layers to our Generator (just uncomment commented line). Restart the Notebook, run again and observe the results. At which epoch you start to see the first shapes of digits instead of pure noise? \n"
      ],
      "metadata": {
        "id": "qBelVjOAE2iR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Run the training with more number of epochs and try to spot typical \"problems\" with GANs with basic architectures:\n",
        "\n",
        "- Non-convergence: the model parameters oscillate, destabilize and never converge,\n",
        "- Mode collapse: the generator collapses which produces limited varieties of samples (e.g. only '9' and '0' digits),\n",
        "- Diminished gradient: the discriminator gets too successful that the generator gradient vanishes and learns nothing,\n",
        "- Unbalance between the generator and discriminator causing overfitting\n",
        "- Highly sensitive to the hyperparameter selections.\n",
        "\n",
        "[Read more:](https://jonathan-hui.medium.com/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b)"
      ],
      "metadata": {
        "id": "900uAYXE1IxO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jASDd4EG1fNq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}