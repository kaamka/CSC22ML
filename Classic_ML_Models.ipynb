{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Classification Models - Basic ML Models\n",
        "\n",
        "Classification is a task that requires the usage of Machine Learning (ML) algorithms that learn how to assign a class label to examples from the problem domain. Examples:\n",
        "- classify if a message is spam or not\n",
        "- given a handwritten character, classify it as one of the known characters\n",
        "\n",
        "READ MORE: [Classification with Machine Learning](https://www.edureka.co/blog/classification-in-machine-learning/)\n",
        "\n",
        "**TASK:** Try some ML classification models for popular task: classify a passenger as surviver (or not) of Titanic catastophe \n",
        "\n",
        "1. Titanic classification with few popular ML models.\n",
        "2. Iris classification \n",
        "\n",
        "**TO DO:** Read and understand the following code. Run the cells, analyse the results and if everything is clear, follow the instructions concerning exercises part."
      ],
      "metadata": {
        "id": "M1wy38wNDGU9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmy9DV6GCZaJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pandas import read_csv\n",
        "import numpy as np\n",
        "from pandas.plotting import scatter_matrix\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ML methods\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "# ML models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXZhTAqWCZaP"
      },
      "source": [
        "# 1. Titanic classification\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ewar_G1CZaR"
      },
      "source": [
        "## 1.1 Load and summarize your data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Xa7-WWUCZaS"
      },
      "outputs": [],
      "source": [
        "#Load the data\n",
        "titanic = sns.load_dataset('titanic')\n",
        "#Print the first 10 rows of data\n",
        "titanic.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "where...\n",
        "\n",
        "**pclass:** A proxy for socio-economic status (SES)\n",
        "1st = Upper\n",
        "2nd = Middle\n",
        "3rd = Lower\n",
        "\n",
        "**age:** Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n",
        "\n",
        "**sibsp:** The dataset defines family relations in this way...\n",
        "Sibling = brother, sister, stepbrother, stepsister\n",
        "Spouse = husband, wife (mistresses and fiancés were ignored)\n",
        "\n",
        "**parch:** The dataset defines family relations in this way...\n",
        "Parent = mother, father\n",
        "Child = daughter, son, stepdaughter, stepson\n",
        "Some children travelled only with a nanny, therefore parch=0 for them.\n",
        "\n",
        "**embarked** Port of Embarkation\n",
        "C = Cherbourg, Q = Queenstown, S = Southampton"
      ],
      "metadata": {
        "id": "Pnwm03tt59FY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The detailed explanation of meaning of columns can be found here: [Kaggle Titanic](https://www.kaggle.com/c/titanic/data)"
      ],
      "metadata": {
        "id": "wK0Jtj7D2HTC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPoTiS6pCZaT"
      },
      "outputs": [],
      "source": [
        "#Count the number of rows and columns in the data set \n",
        "titanic.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9bMKLXWCZaU"
      },
      "source": [
        "Show the data as a DataFrame table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qm9PMk9PCZaV"
      },
      "outputs": [],
      "source": [
        "titanic.describe()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Get a count of the number of survivors  \n",
        "titanic['survived'].value_counts()"
      ],
      "metadata": {
        "id": "3-qEAUNfFYPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_kt1G3eCZaY"
      },
      "source": [
        "## 1.2. Visualize your data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCIc9pO_CZaa"
      },
      "outputs": [],
      "source": [
        "#Visualize the count of number of survivors\n",
        "sns.countplot(titanic['survived'],label=\"Count\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the count of survivors for columns 'who', 'sex', 'pclass', 'sibsp', 'parch', and 'embarked'\n",
        "cols = ['who', 'sex', 'pclass', 'sibsp', 'parch', 'embarked']\n",
        "\n",
        "n_rows = 2\n",
        "n_cols = 3\n",
        "\n",
        "fig, axs = plt.subplots(n_rows, n_cols, figsize=(12,7))\n",
        "\n",
        "for r in range(0,n_rows):\n",
        "    for c in range(0,n_cols):  \n",
        "        \n",
        "        i = r*n_cols+ c #index to go through the number of columns       \n",
        "        ax = axs[r][c] #Show where to position each subplot\n",
        "        sns.countplot(titanic[cols[i]], hue=titanic[\"survived\"], ax=ax)\n",
        "        ax.set_title(cols[i])\n",
        "        ax.legend(title=\"survived\", loc='upper right') \n",
        "        \n",
        "plt.tight_layout()   #tight_layout"
      ],
      "metadata": {
        "id": "U26N3eR_FkW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Look at survival rate by sex and class\n",
        "titanic.pivot_table('survived', index='sex', columns='class')"
      ],
      "metadata": {
        "id": "0kZqy3HUFw2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the survival rate of each class.\n",
        "sns.barplot(x='class', y='survived', data=titanic)"
      ],
      "metadata": {
        "id": "6cwC6veVF83v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Look at survival rate by sex, age and class\n",
        "age = pd.cut(titanic['age'], [0, 18, 80])\n",
        "titanic.pivot_table('survived', ['sex', age], 'class')"
      ],
      "metadata": {
        "id": "AIQngmm0GALF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWpfG3nFCZab"
      },
      "source": [
        "## 1.3. Preparing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zqq5YWxjCZab"
      },
      "source": [
        "Typical data pre-processing for Machine Learning task may indclude following steps:\n",
        "\n",
        "- Handling missing data (eliminating/estimating missing data)\n",
        "- Dealing with unbalanced classes\n",
        "- Feature encoding\n",
        "- Feature scaling (normalization/standardization)\n",
        "- Extract independent values\n",
        "- Test / Validation Split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCw2rJJICZab"
      },
      "source": [
        "### Handing missing data \n",
        "\n",
        "Check which columns contain empty values (NaN, NAN, na). Looks like columns age, embarked, deck, and embarked_town are missing some values.\n",
        "\n",
        "All the other columns are not missing any values."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Count the empty (NaN, NAN, na) values in each column \n",
        "titanic.isna().sum()"
      ],
      "metadata": {
        "id": "nw37z36BGQzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's drop the redundant columns that are non-numerical and remove rows with missing values."
      ],
      "metadata": {
        "id": "nPXU5HoBGkfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the columns\n",
        "try:\n",
        "    titanic = titanic.drop(['deck', 'embark_town', 'alive', 'class', 'alone', 'adult_male', 'who'], axis=1)\n",
        "except KeyError:\n",
        "    print(\"Haven't you already run this cell? Redundant columns are already removed!\")\n",
        "\n",
        "#Remove the rows with missing values\n",
        "titanic = titanic.dropna(subset =['embarked', 'age'])"
      ],
      "metadata": {
        "id": "XT9be4MQGfCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Count the NEW number of rows and columns in the data set\n",
        "titanic.shape"
      ],
      "metadata": {
        "id": "A71zxj4AG9WG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titanic\n"
      ],
      "metadata": {
        "id": "Q5T2y37556zC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature encoding\n",
        "\n"
      ],
      "metadata": {
        "id": "2mPTI7BiHdtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titanic.dtypes"
      ],
      "metadata": {
        "id": "P8j31NhZH19_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Print the unique values in the columns\n",
        "print(titanic['sex'].unique())\n",
        "print(titanic['embarked'].unique())"
      ],
      "metadata": {
        "id": "XoNGXLlcIOr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change the non-numeric data to numeric data, and print the new values."
      ],
      "metadata": {
        "id": "eD3n51cJISSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Encoding categorical data values (Transforming object data types to integers)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder = LabelEncoder()\n",
        "\n",
        "#Encode sex column\n",
        "titanic.iloc[:,2]= labelencoder.fit_transform(titanic.iloc[:,2].values)\n",
        "#print(labelencoder.fit_transform(titanic.iloc[:,2].values))\n",
        "\n",
        "#Encode embarked\n",
        "titanic.iloc[:,7]= labelencoder.fit_transform(titanic.iloc[:,7].values)\n",
        "#print(labelencoder.fit_transform(titanic.iloc[:,7].values))\n",
        "\n",
        "#Print the NEW unique values in the columns\n",
        "print(titanic['sex'].unique())\n",
        "print(titanic['embarked'].unique())"
      ],
      "metadata": {
        "id": "qLan_-UzITpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XG43QSkCZad"
      },
      "source": [
        "### Extract dependent/independent values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7rLtUXaCZad"
      },
      "outputs": [],
      "source": [
        "#Split the data into independent 'X' and dependent 'Y' variables\n",
        "X = titanic.iloc[:, 1:8].values # all columns\n",
        "Y = titanic.iloc[:, 0].values # survived column\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y"
      ],
      "metadata": {
        "id": "rZCBL9bm6m9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kw5v1LWVCZae"
      },
      "source": [
        "### Test / Validation Split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2giNMs4CZae"
      },
      "source": [
        "- x_train – features for the training data\n",
        "- x_test – features for the test data\n",
        "- y_train – labels for training data\n",
        "- y_test – labels for testing data\n",
        "\n",
        "\n",
        "train_test_split() method can handle both: numpy arrays or DataFrames:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Y6_EoZWCZaf"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into 80% Training set and 20% Testing set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOby7Zk7CZag"
      },
      "outputs": [],
      "source": [
        "print(\"Features (independent variables):\")\n",
        "print(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hD-gOAECZag"
      },
      "outputs": [],
      "source": [
        "print(\"Corresponding labels (dependent variables):\")\n",
        "print(Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ev0OVGOXCZag"
      },
      "source": [
        "## 1.4. Building models \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COy7G4IoCZag"
      },
      "source": [
        "### Build Models\n",
        "\n",
        "\n",
        "Let’s test 6 different algorithms (models):\n",
        "\n",
        "- Logistic Regression (LR)\n",
        "- K Nearest Neighbor (KNN)\n",
        "- Support Vector Machine (SVM)\n",
        "- Gaussian Naive Bayes (NB)\n",
        "- Decision Tree Classifier \n",
        "- Random Forest Classifier \n",
        "  \n",
        "\n",
        "This is a good mixture of simple linear (LR and LDA), nonlinear (KNN, CART, NB and SVM) algorithms."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = []\n",
        "model.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\n",
        "model.append(('KNN', KNeighborsClassifier()))\n",
        "model.append(('SVM', SVC(gamma='auto')))\n",
        "model.append(('NB', GaussianNB()))\n",
        "model.append(('DT', DecisionTreeClassifier()))\n",
        "model.append(('RF', RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CxayV7ub9Tnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for nm, mdl in model:\n",
        "    mdl.fit(X_train, Y_train)\n",
        "    print(nm, mdl.score(X_train, Y_train))"
      ],
      "metadata": {
        "id": "PPyPz4aGD4r2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cRMEMY2CZam"
      },
      "source": [
        "## 1.5. Select the best model \n",
        "\n",
        "We now have 6 models and accuracy estimations for each. We need to compare the models to each other and select the most accurate.\n",
        "\n",
        "The model that was most accurate on the train data is the Logistic Regression Model with an accuracy of 80.32%.\n",
        "\n",
        "Let's test the models with testing data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix \n",
        "import seaborn as sns\n",
        "\n",
        "for i in range(len(model)):\n",
        "    cm = confusion_matrix(Y_test, model[i][1].predict(X_test)) \n",
        "    #extracting TN, FP, FN, TP\n",
        "    TN, FP, FN, TP = confusion_matrix(Y_test, model[i][1].predict(X_test)).ravel()\n",
        "    #print(cm)\n",
        "    plt.figure()  \n",
        "    p = sns.heatmap(cm, annot=True)\n",
        "    title = f'Mode {model[i][0]} Testing Accuracy = \"{(TP + TN) / (TP + TN + FN + FP)} !\"'\n",
        "    p.set_title(title)\n",
        "    "
      ],
      "metadata": {
        "id": "BhwL64QELHmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest Classifier did second-best on the training and testing data and has an accuracy of 80.41% on the testing data and 97.53% on the training data.\n",
        "\n",
        "Now we can get the important features."
      ],
      "metadata": {
        "id": "vSrxLlN1M4K0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the importance of the features\n",
        "forest = model[5][1]\n",
        "importances = pd.DataFrame({'feature':titanic.iloc[:, 1:8].columns,'importance':np.round(forest.feature_importances_,3)})\n",
        "importances = importances.sort_values('importance',ascending=False).set_index('feature')\n",
        "importances"
      ],
      "metadata": {
        "id": "Pu35cEzSM3mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualize the importance\n",
        "importances.plot.bar()"
      ],
      "metadata": {
        "id": "2YQSqFYlOAuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXyolVqhCZan"
      },
      "source": [
        "## 1.7. Algorithm Tuning\n",
        "\n",
        "Grid search is a process that searches exhaustively through a manually specified subset of the hyperparameter space of the targeted algorithm. Random search, on the other hand, selects a value for each hyperparameter independently using a probability distribution.\n",
        "\n",
        "Let's try to tune the RandomForest model with using of both methods.\n",
        "\n",
        "### Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of trees in random forest\n",
        "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 80, num = 10)]\n",
        "# Number of features to consider at every split\n",
        "max_features = ['auto', 'sqrt']\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [2,4]\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2]\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True, False]\n",
        "\n",
        "\n",
        "# Create the param grid\n",
        "param_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap}\n",
        "print(param_grid)\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf_Model = RandomForestClassifier()\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "rf_Grid = GridSearchCV(estimator = rf_Model, param_grid = param_grid, cv = 3, verbose=2, n_jobs = 4)\n",
        "rf_Grid.fit(X_train, Y_train)"
      ],
      "metadata": {
        "id": "OvLoc84CN6_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_params = rf_Grid.best_params_\n",
        "best_params"
      ],
      "metadata": {
        "id": "B7A6NO7HOaDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (f'Train Accuracy - : {rf_Grid.score(X_train,Y_train):.3f}')\n",
        "print (f'Test Accuracy - : {rf_Grid.score(X_test,Y_test):.3f}')"
      ],
      "metadata": {
        "id": "THZQUdt-Oyyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: Why default hyperparameters gave better results than those calculated with Grid Search?\n",
        "\n",
        "Because grid search creates subsamples of the data repeatedly. That means the model is trained on 80% of x_train in each iteration and the results are the mean of predictions on the other 20%. Read more: `missing link`"
      ],
      "metadata": {
        "id": "QHJB69yhPBbb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Search"
      ],
      "metadata": {
        "id": "ewlMol2zToTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf_Model = RandomForestClassifier()\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "rf_RandomGrid = RandomizedSearchCV(estimator = rf_Model, param_distributions = param_grid, cv = 3, verbose=2, n_jobs = 4)\n",
        "\n",
        "rf_RandomGrid.fit(X_train, Y_train)\n"
      ],
      "metadata": {
        "id": "SnREuMEDTq6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_params_rand = rf_RandomGrid.best_params_\n",
        "best_params_rand"
      ],
      "metadata": {
        "id": "MJQDSia9LjRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (f'Train Accuracy - : {rf_RandomGrid.score(X_train,Y_train):.3f}')\n",
        "print (f'Test Accuracy - : {rf_RandomGrid.score(X_test,Y_test):.3f}')"
      ],
      "metadata": {
        "id": "2sf_RrflLgFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise\n",
        "\n",
        "1. Try to experiment with above example - change ranges? Are you able to improve the accuracy?\n",
        "\n",
        "2. We could also perform this procedure for all models. In order to prepare parameters for tuning you may read about models implementation in scikit-learn documentation. Look at [KNeighborsClassifier]('https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) and try to tune this algorithm using Grid Search or Random Search method in order to obtain better results!"
      ],
      "metadata": {
        "id": "BiaW4bMuoKtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE"
      ],
      "metadata": {
        "id": "W6X7h49B7_AF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECiIzyubCZao"
      },
      "source": [
        "## 1.8. Make and evaluate predictions\n",
        "\n",
        "We can fit the model on the entire training dataset and make predictions on the validation dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpS_n_plCZao"
      },
      "outputs": [],
      "source": [
        "final_model = RandomForestClassifier(**best_params) \n",
        "final_model.fit(X_train, Y_train)\n",
        "print(final_model.score(X_train, Y_train))\n",
        "final_predictions = final_model.predict(X_test)\n",
        "#final_model.classes_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzAFaeF2CZap"
      },
      "source": [
        "### Make predictions on a custom sample\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5U3QRam8CZap"
      },
      "outputs": [],
      "source": [
        "my_survival_d = {'pclass':1, \n",
        "               'sex': 0, \n",
        "               'age':28, \n",
        "               'sibsp':0, \n",
        "               'parch':1, \n",
        "               'fare': 91.2833, \n",
        "               'embarked': 1}\n",
        "\n",
        "my_survival = [[*my_survival_d.values()]]\n",
        "\n",
        "\n",
        "my_models = [mdl for nm, mdl in model] + [final_model]\n",
        "\n",
        "#print(my_survival)\n",
        "#Print Prediction of your model\n",
        "for mod in my_models:\n",
        "    pred = mod.predict(my_survival)\n",
        "    print(pred)\n",
        "    print(f'Due to {mod} prediction:')\n",
        "    if pred == 0:\n",
        "        print('Oh no! You are dead!')\n",
        "    else:\n",
        "        print('You survived')\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise**\n",
        "\n",
        "Put your values into ``` my_survival_d ```\n",
        "and find out if you would have had survided in the disaster or not due to all our prepared models. You can play with other values. Are you satisfied with the results?\n"
      ],
      "metadata": {
        "id": "GR-Ya5qmrVnd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IvPTAS6jKMiZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "e5850eac91a2827521610ccf5c79e7bfd6182e4ab8e286c9de1db8bcf02b2062"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit ('ml': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
