{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Artifficial Neural Networks\n",
        "\n",
        "In this Notebook we will take a look at the core of modern Machine Learning  - **Artifficial Neural Networks**. \n",
        "This area of Machine Learning, where Artifficial Neural Networks have complex architecture with big number of hidden layers is often defined as Deep Learning.\n",
        "\n",
        "**Deep learning:** *A subfield of machine learning that structures algorithms in layers to create an “artificial neural network” that can learn and make intelligent decisions on its own.*\n",
        "\n",
        "Artifficial Neural Networks (ANN) can be designed to deal with many different tasks. \n",
        "In this Notebook we will work with **classification**. \n",
        "\n",
        "**TASK:** Train the simple  Neural Network to classify handwritten digit as a proper digit.  \n",
        "\n",
        "As A training data we will be using popular dataset - MNIST, containing handwritten digits (0-9) images with correct labels.\n",
        "[Read more about MNIST dataset](http://yann.lecun.com/exdb/mnist/).\n",
        "\n",
        "**WHAT YOU WILL LEARN:**\n",
        "\n",
        "- how to prepare data for training with Pytorch (DataLoader, batches)\n",
        "- what are main components of basic Artifficial Neural Network\n",
        "- how to implement basic Artifficial Neural Network with Pytorch\n",
        "- how to perform main training loop in Pytorch\n",
        "- what are Hyperparameters for ANN and how to tune them\n",
        "\n",
        "\n",
        "**TO DO:** Read and understand following code. Run the cells, analyse the results and if everything is clear, follow the instructions concerning exercises parts. \n"
      ],
      "metadata": {
        "id": "zzgbj8uaM7nQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification with Artifficial Neural Network"
      ],
      "metadata": {
        "id": "mJ00uFfDotB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preperation and Visualization\n",
        "\n",
        "- download data\n",
        "- transform Images to Tensors\n",
        "- create DataLoader with defined batch size\n",
        "\n",
        "The MNIST database of handwritten digits has a training set of 60,000 examples, and a test set of 10,000 examples. The digits have been size-normalized and centered in a fixed-size image. \n",
        "\n",
        "MNIST dataset is available within `torchvision.datasets` package! \n",
        "You can download them with single line of code. \n",
        "As you can see you there is no need to split data into \"train\" and \"test\" manually - you just need to specify a parameter \"train\" as True or False.\n"
      ],
      "metadata": {
        "id": "ARwuemmTNPQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "train = datasets.MNIST('', train=True, download=True,)\n",
        "\n",
        "test = datasets.MNIST('', train=False, download=True,)\n",
        "\n",
        "\n",
        "print(type(train[0][0]))\n",
        "plt.imshow(train[0][0])"
      ],
      "metadata": {
        "id": "LqUfp9xsqeng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The important feature here is `transform`, wich enables to perform some changes to the data, like normalization, scaling etc. We need to convert the data into Tensors, which is required data type for working with Pytorch. \n",
        "\n",
        "### **Exercise** \n",
        "\n",
        "Lets download the MNIST dataset once again. Uncomment the part concerning `transforms` and see how the data type has changed."
      ],
      "metadata": {
        "id": "TXe4q7WFqbZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = datasets.MNIST('', train=True, download=True,\n",
        "                    #    transform=transforms.Compose([\n",
        "                    #        transforms.ToTensor()\n",
        "                    #  ])\n",
        "                       )\n",
        "\n",
        "test = datasets.MNIST('', train=False, download=True,\n",
        "                    #    transform=transforms.Compose([\n",
        "                    #        transforms.ToTensor()\n",
        "                    # ])\n",
        "                    )\n",
        "\n",
        "print(type(train[0][0]))\n",
        "print(train[0][0])\n",
        "plt.imshow(train[0][0].view(28,28))\n"
      ],
      "metadata": {
        "id": "AvW6Ok8aouLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to to make the data iterable using [DataLoader](https://pytorch.org/docs/stable/data.html) module in order to be able grab the batches of data (instead of whole dataset) for training or exploring purposes."
      ],
      "metadata": {
        "id": "IRdr0HUNnBcG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
        "testset = torch.utils.data.DataLoader(test, batch_size=10, shuffle=False)\n"
      ],
      "metadata": {
        "id": "_6suDn2nnDGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Batches - why do we need them?**\n",
        "With batches, instead of loading all the 60 000 images into memory which is way too expensive for the computer, you can load 64 images(1 batch) for 938 times in 1 epoch of training which requires way less memory as compared to loading the complete data set."
      ],
      "metadata": {
        "id": "H_Leea-mmIGe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise**\n",
        "\n",
        "Since we have created DataLoaders we now can iterate over them.\n",
        "In the the next cell, there is the loop for iterating over `trainset` and inspect it. Do the same with `testset`!"
      ],
      "metadata": {
        "id": "5fiUjqlPCQht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for data in trainset:\n",
        "    batch_of_images = data[0]\n",
        "    print(batch_of_images.size())\n",
        "    batch_of_labels = data[1]\n",
        "    print(batch_of_labels.size())\n",
        "\n",
        "    fig = plt.figure()\n",
        "    fig.set_size_inches(18.5, 10.5, forward=True)\n",
        "    for i, (image, label) in enumerate(zip(batch_of_images,batch_of_labels)):\n",
        "        plt.subplot(2,5,i+1)\n",
        "        plt.imshow(image.view(28,28), cmap='gray', interpolation='none')\n",
        "        plt.title(f\"Ground Truth: {label}\")\n",
        "    fig\n",
        "\n",
        "    break"
      ],
      "metadata": {
        "id": "04hH0RfkEj2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### for data in testset:\n",
        "    ### YOUR CODE HERE"
      ],
      "metadata": {
        "id": "Oj4cG6jfEkY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Buld Neural Network Model"
      ],
      "metadata": {
        "id": "np8C5C9LOt-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n"
      ],
      "metadata": {
        "id": "f8eSo7rpO7Dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Typical Neral Network in Pytorch inhereits from the `nn.Module class` and has the following structure:\n"
      ],
      "metadata": {
        "id": "gE8CpNdoHGIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "net = Net()\n",
        "print(net)"
      ],
      "metadata": {
        "id": "sgceZGHiHFmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time to put some layers inside the Network!\n",
        "\n",
        "\n",
        "Pay attention to parameters of layers - they have to correspond to our input dimensions and expected output dimensions!\n"
      ],
      "metadata": {
        "id": "1q2gj4ofHd-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 64) # 28x28 is our image size; however basic neural network is going to expect to have a flattened array, so not a 28*28 = 784.\n",
        "        self.fc2 = nn.Linear(64, 64) # hidden layer - next layer is always going to accept however many connections the previous layer outputs\n",
        "        self.fc3 = nn.Linear(64, 64) # hidden layer\n",
        "        # may be more hidden layers - the more layer, the more \"deep\" our Network is\n",
        "        self.fc4 = nn.Linear(64, 10) # output layer needs 10 neurons because we have 10 classes!\n",
        "\n",
        "    # passing our data through the layers + activations\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x)) # simply passing our data (x) through the layer with ReLU activation (activation functions are keeping our data scaled between 0 and 1)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return F.log_softmax(x, dim=1) # softmax is good for multi-class classification - outputs themselves are a confidence score, adding up to 1\n",
        "\n",
        "net = Net()\n",
        "print(net)"
      ],
      "metadata": {
        "id": "LTGoLRVNHiJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another way to construct the Network is with using `nn.Sequential`.\n",
        "Useful especially when the architecture is more complex.\n",
        "\n",
        "Also different way for adding ReLU activation is presented. Now we have it in the form of ReLU() layers. "
      ],
      "metadata": {
        "id": "zyouO5AtzaBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(28*28, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 10) )\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layers(x)\n",
        "        return F.log_softmax(x, dim=1) \n",
        "\n",
        "net = Net()\n",
        "print(net)"
      ],
      "metadata": {
        "id": "vUu-K6JDxktC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets verify if out Network is correctly designed. We will create a random image with 28x28 dimensions and pass it to out Network:"
      ],
      "metadata": {
        "id": "TLh42lyqJ_5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.randn((28,28)) # create random 28x28 image\n",
        "print('Input:')\n",
        "plt.imshow(X.view(28,28))\n",
        "plt.show()\n",
        "\n",
        "\n",
        "X = X.view(-1,28*28) # neural network wants this to be flattened\n",
        "output = net(X)\n",
        "print(f'Output: {output}')"
      ],
      "metadata": {
        "id": "lzDzPHQkJFrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise**\n",
        "\n",
        "Now verify the Network with one of data from our dataset. \n",
        "Hint: Take a batch from out `trainset` using `next(iter(trainset))`."
      ],
      "metadata": {
        "id": "k6TX3RnQMUHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### YOUR CODE HERE"
      ],
      "metadata": {
        "id": "Hc1Z2sQAL0gZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n"
      ],
      "metadata": {
        "id": "WyI03ZZmPYyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets start the main part - trainig!\n",
        "\n",
        "In order to perform the training, apart from the Model architecture itself we need 2 more elements:\n",
        "- loss function - calculates \"how far off\" our classifications are from reality\n",
        "- optimizer - adjusts our model's adjustable parameters like the weights, to slowly, over time, fit our data\n",
        "\n"
      ],
      "metadata": {
        "id": "7OCT4K7OM-S7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "tMNYEyKtN4SS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main training loop in Pytorch requires following actions:\n",
        "- loop by epochs\n",
        "- iterating through the DataLoader (batch by batch)\n",
        "- setting gradients to 0 before calculating the loss function\n",
        "- pass the batch of the data through the Model in order to obtain the output\n",
        "- compare the output with true labels - calculate the loss\n",
        "- apply this loss backwards to update the parameters\n",
        "- optimize the weights of neurons with optimizer algorithm\n",
        "- optional - collect the loss and accuracy values at each step in order to plot them later"
      ],
      "metadata": {
        "id": "n8Sqxa2ck8c7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 3\n",
        "loss_list_epoch = []\n",
        "\n",
        "for epoch in range(epochs): # 3 full passes over the data\n",
        "    for data in trainset:  # 'data' is a batch of data\n",
        "        X, y = data  # X is the batch of features, y is the batch of targets.\n",
        "        net.zero_grad()  # sets gradients to 0 before loss calc. You will do this likely every step.\n",
        "        output = net(X.view(-1,784))  # pass in the reshaped batch (recall they are 28x28)\n",
        "        #loss = F.nll_loss(output, y)  # calc and grab the loss value - The negative log likelihood loss\n",
        "        loss = loss_function(output, y)\n",
        "        loss.backward()  # apply this loss backwards through the network's parameters\n",
        "        optimizer.step()  # attempt to optimize weights to account for loss/gradients\n",
        "    loss_list_epoch.append(loss.item())\n",
        "    print(f'Epoch: {epoch}; Loss: {loss}')  # print loss for each epoch - we hope loss (a measure of wrongness) declines!\n",
        "\n",
        "# Plot the loss value per epoch\n",
        "plt.plot(range(epochs), loss_list_epoch)\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.title(\"Loss by epoch\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "P6OJasbsPDeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accuracy value and evaluation on examples from dataset\n",
        "\n",
        "Lets calculate our final accuracy value on `testset` and test our Model on examplary image from `testset`!"
      ],
      "metadata": {
        "id": "DnU_eFcJhseV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testset:\n",
        "        X, y = data\n",
        "        output = net(X.view(-1,784))\n",
        "        #print(output)\n",
        "        for idx, i in enumerate(output):\n",
        "            #print(torch.argmax(i), y[idx])\n",
        "            if torch.argmax(i) == y[idx]:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "\n",
        "print(\"Accuracy: \", round(correct/total, 3))"
      ],
      "metadata": {
        "id": "Hx9svt78gAfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print('Input:')\n",
        "plt.imshow(X[0].view(28,28))\n",
        "plt.show()\n",
        "\n",
        "output = torch.argmax(net(X[0].view(-1,784))[0])\n",
        "print(f'Output: {output}')"
      ],
      "metadata": {
        "id": "v915CxWPgA1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise**\n",
        "\n",
        "Try to check more examples! Can you find the one which is 'difficult' for our Model to classify?"
      ],
      "metadata": {
        "id": "nF_OiXyCmLSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE"
      ],
      "metadata": {
        "id": "q5DegvjrgA3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters\n",
        "\n",
        "Tuning Neural Network is basically testing different values of hyperparameters which are:\n",
        "\n",
        "- BATCH SIZE\n",
        "- LEARNING RATE\n",
        "- NUMBER OF HIDDEN LAYERS\n",
        "- NUMBER OF EPOCHS\n"
      ],
      "metadata": {
        "id": "_CAY8bd3OTQd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The good practice is to all Hyperparameters as global variables at the beginning of the Notebook as follows:\n"
      ],
      "metadata": {
        "id": "B2mu0ZV_bWbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 0.001\n",
        "OPTIMIZER = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
        "NUMBER_OF_HIDDEN_LAYERS = 5\n",
        "EPOCHS = 6"
      ],
      "metadata": {
        "id": "fpz5LUk4OX1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise**\n",
        "\n",
        "Try to refactor the code for Network Model architecture assuming that `NUMBER_OF_HIDDEN_LAYERS` is a parameter. \n",
        "\n",
        "**Hint**: Use `nn.Sequential`.\n",
        "\n",
        "Verify your Network  with 28x28 input image as we did it previously."
      ],
      "metadata": {
        "id": "oMV3CRjcUTDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUMBER_OF_HIDDEN_LAYERS = 5\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 64) \n",
        "        #### YOUR CODE HERE \n",
        "        self.fc_n = nn.Linear(64, 10) \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x)) \n",
        "        #### YOUR CODE HERE\n",
        "        x = self.fc_n(x)\n",
        "        return F.log_softmax(x, dim=1) \n",
        "\n",
        "net = Net()\n",
        "print(net)"
      ],
      "metadata": {
        "id": "_boLTZYHU9FA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check your Model\n",
        "\n",
        "### YOUR CODE HERE"
      ],
      "metadata": {
        "id": "Mt4PZDEbeSKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise** \n",
        "\n",
        "1. (Optional) Refactor the code in the Notebook to keep Hyperparameters as global variables.\n",
        "2. Change the learning rate value to something much smaller (e.g. 0.00000001) and run the training again. What are the results? \n",
        "3. (Optional - if you have time) Leave this small value of learning rate but try to \"fix\" the results by changing other hyperparameters values."
      ],
      "metadata": {
        "id": "5FKY6I494zn-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EAC2Nwft7jIt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}